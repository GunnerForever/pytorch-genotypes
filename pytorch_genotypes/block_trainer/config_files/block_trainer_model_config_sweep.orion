def get_block_trainer_config():
    lr = lr~loguniform(2e-5, 1e-3)
    fidelity = fidelity~fidelity(low=1, high=100)

    max_epochs = min(100 * fidelity, 500)

    weight_decay = weight_decay~loguniform(1e-10, 1e-3)

    input_dropout_p = input_dropout_p~uniform(0, 0.2)
    enc_h_dropout_p = enc_h_dropout_p~uniform(0, 0.5)

    enc_h1 = model/enc_h1~uniform(64, 2000, discrete=True)
    enc_h2 = model/enc_h2~uniform(64, 2000, discrete=True)

    dec_h1 = model/dec_h1~uniform(64, 2000, discrete=True)
    dec_h2 = model/dec_h2~uniform(64, 2000, discrete=True)


    return {
        "lr": lr,
        "max_epochs": max_epochs,
        "batch_size": 256,
        "weight_decay": weight_decay,
        "add_batchnorm": False,
        "input_dropout_p": input_dropout_p,
        "enc_h_dropout_p": enc_h_dropout_p,
        "dec_h_dropout_p": None,
        "val_proportion": 0.1,
        "use_standardized_genotype": False,
        "model/activation": "GELU",
        "model/rep_size": 128,
        "model/enc_layers": [h for h in (enc_h1, enc_h2)],
        "model/dec_layers": [h for h in (dec_h1, dec_h2)]
    }
